---
---
References
==========

@inproceedings{Dinits1970AlgorithmFS,
  title={Algorithm for solution of a problem of maximal flow in a network with power estimation},
  author={E. A. Dinits},
  year={1970}
}


@inproceedings{bahdanau2014neural,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  booktitle={proceedings of the 2015 International Conference on
Learning Representations},
  year={2015},
}

@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={proceedings of International Conference on Machine Learning},
  pages={2048--2057},
  year={2015}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{rocktaschel2015reasoning,
  title={Reasoning about Entailment with Neural Attention},
  author={Rockt{\"a}schel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Kocisky, Tomas and Blunsom, Phil},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  booktitle={proceedings of the 2019 International Conference on Learning Representations},
  url={https://arxiv.org/abs/1807.03819},
  year={2019}
}

@article{chen2019improving,
  title={Improving the Interpretability of Neural Sentiment Classifiers via Data Augmentation},
  author={Chen, Hanjie and Ji, Yangfeng},
  journal={arXiv preprint arXiv:1909.04225},
  year={2019}
}

@inproceedings{serrano2019attention,
    title = "Is Attention Interpretable?",
    author = "Serrano, Sofia  and
      Smith, Noah A.",
    booktitle = "proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1282"
}

@inproceedings{wang2016attention,
  title={Attention-based LSTM for aspect-level sentiment classification},
  author={Wang, Yequan and Huang, Minlie and Zhao, Li and others},
  booktitle={proceedings of the 2016 conference on empirical methods in natural language processing},
  pages={606--615},
  year={2016}
}

@inproceedings{lee2017interactive,
  title={Interactive visualization and manipulation of attention-based neural machine translation},
  author={Lee, Jaesong and Shin, Joong-Hwi and Kim, Jun-Seok},
  booktitle={proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={121--126},
  year={2017}
}

@inproceedings{jain2019attention,
title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    booktitle = "proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1357",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
}


@article{pruthi2019learning,
  title={Learning to deceive with attention-based explanations},
  author={Pruthi, Danish and Gupta, Mansi and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C},
  journal={arXiv preprint arXiv:1909.07913},
  year={2019}
}

@article{EdmondsKarp,
 author = {Edmonds, Jack and Karp, Richard M.},
 title = {Theoretical Improvements in Algorithmic Efficiency for Network Flow Problems},
 journal = {J. ACM},
 issue_date = {April 1972},
 volume = {19},
 number = {2},
 month = apr,
 year = {1972},
 issn = {0004-5411},
 pages = {248--264},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/321694.321699},
 doi = {10.1145/321694.321699},
 acmid = {321699},
 publisher = {ACM},
}

@inproceedings{wiegreffe2019attention,
    title = "Attention is not not Explanation",
    author = "Wiegreffe, Sarah  and
      Pinter, Yuval",
    booktitle = "proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1002",
}

@article{vashishth2019attention,
  title={Attention interpretability across nlp tasks},
  author={Vashishth, Shikhar and Upadhyay, Shyam and Tomar, Gaurav Singh and Faruqui, Manaal},
  journal={arXiv preprint arXiv:1909.11218},
  year={2019}
}

@article{coenen2019visualizing,
  title={Visualizing and Measuring the Geometry of BERT},
  author={Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1906.02715},
  year={2019}
}


@article{vig2019visualizing,
  title={Visualizing Attention in Transformer-Based Language models},
  author={Vig, Jesse},
  journal={arXiv preprint arXiv:1904.02679},
  year={2019}
}


@inproceedings{brunner2019validity,
    title={On Identifiability in Transformers},
    author={Gino Brunner and Yang Liu and Damian Pascual and Oliver Richter and Massimiliano Ciaramita and Roger Wattenhofer},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=BJg1f6EFDB}
}


@incollection{montavon2019layer,
  title={Layer-wise relevance propagation: an overview},
  author={Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  booktitle={Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  pages={193--209},
  year={2019},
  publisher={Springer}
}

@article{linzen2016assessing,
    Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
    Journal = {Transactions of the Association for Computational Linguistics},
    Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
    Year = {2016}
}


@article{wolf2019huggingfacests,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  year={2019}
}

@inproceedings{devlin2018bert,
 title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
}

@inbook{ancona2019,
author="Ancona, Marco
and Ceolini, Enea
and {\"O}ztireli, Cengiz
and Gross, Markus",
title="Gradient-Based Attribution Methods",
bookTitle="Explainable AI: Interpreting, Explaining and Visualizing Deep Learning",
year="2019",
publisher="Springer International Publishing",
pages="169--191",
}

@inproceedings{alg-maxflow,
 author = {Orlin, James B.},
 title = {Max Flows in O(Nm) Time, or Better},
 booktitle = {proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing},
 series = {STOC '13},
 year = {2013},
 isbn = {978-1-4503-2029-0},
 pages = {765--774},
 numpages = {10},
 publisher = {ACM},
}

@book{clrs,
 author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
 title = {Introduction to Algorithms, Third Edition},
 year = {2009},
 isbn = {0262033844, 9780262033848},
 edition = {3rd},
 publisher = {The MIT Press},
}

@misc{sanh2019distilbert,
    title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
    author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
    year={2019},
    eprint={1910.01108},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}


@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset.",
}
