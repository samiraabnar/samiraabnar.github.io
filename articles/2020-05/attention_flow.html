<!DOCTYPE html>
<html lang="en">






<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="In our paper, "Quantifying Attention Flow In Transformers", we show that compared to raw attention weights, the token attentions from Attention Rollout and A...">
  <meta name="keywords" content="blog">
  <meta name="author" content="Quantifying Attention Flow in Transformers | Samira Abnar">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Quantifying Attention Flow in Transformers | Samira Abnar">
  <meta name="twitter:description" content="In our paper, &quot;Quantifying Attention Flow In Transformers&quot;, we show that compared to raw attention weights, the token attentions from Attention Rollout and A...">
  
    <meta property="twitter:image" content="samiraabnar.github.io/img/flow_images/attention_flow.gif">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="samiraabnar.github.io/articles/2020-05/attention_flow">
  <meta property="og:title" content="Quantifying Attention Flow in Transformers | Samira Abnar">
  <meta property="og:description" content="In our paper, &quot;Quantifying Attention Flow In Transformers&quot;, we show that compared to raw attention weights, the token attentions from Attention Rollout and A...">
  
    <meta property="og:image" content="samiraabnar.github.io/img/flow_images/attention_flow.gif">
  
  <title>Quantifying Attention Flow in Transformers | Samira Abnar</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="samiraabnar.github.io/css/font-awesome.min.css">
  <link rel="stylesheet" href="samiraabnar.github.io/css/main.css">

  <link rel="canonical" href="samiraabnar.github.io/articles/2020-05/attention_flow">
  <link rel="alternate" type="application/rss+xml" title="Samira Abnar" href="samiraabnar.github.io/feed.xml" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="samiraabnar.github.io/blown.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="samiraabnar.github.io/blown.png">
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="samiraabnar.github.io/">
    <img src="samiraabnar.github.io/img/samira-logo.jpg" alt="" class="avatar">
  </a>
  
  <a href="samiraabnar.github.io/" class="author_name">Samira Abnar</a>
  <span class="author_job">PhD Candidate</span>
  <span class="author_bio mbm">University of Amsterdam</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="samiraabnar.github.io/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="/about">About</a>
      </li>
        
      <li class="nav-item">
        <a href="/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="/tags/">Tags</a>
      </li>
         
    </ul>
  </nav>
  <script type="text/javascript">
  // based on http://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
    <li><a href="http://twitter.com/samiraabnar#username" class="social-link-item" target="_blank"><i class="fa fa-fw fa-twitter"></i></a></li>
    
    
    <li><a href="http://linkedin.com/in/samiraabnar" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    <li><a href="http://instagram.com/samiraabnar" class="social-link-item" target="_blank"><i class="fa fa-fw fa-instagram"></i></a></li>
    
    <li><a href="http://github.com/samiraabnar" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "samiraabnar.github.io/" >
  Home
</a>
<!-- 
<div class="post-image-feature">
  <img class="feature-image" src=
  
  "samiraabnar.github.io/img/flow_images/attention_flow.gif"
  
  alt="Quantifying Attention Flow in Transformers feature image">

  
</div>
 -->


<div id="post">
  <header class="post-header">
    <h1 title="Quantifying Attention Flow in Transformers">Quantifying Attention Flow in Transformers</h1>
    <span class="post-meta">
      <span class="post-date">
        16 MAY 2020
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    6 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <p>Attention <a class="citation" href="#bahdanau2014neural">(Bahdanau et al., 2015; Vaswani et al., 2017)</a> has become the key building block of neural sequence processing models,
and visualizing attention weights is the easiest and most popular approach to interpret a model’s decisions and to gain insights about its internals.
Although it is wrong to equate attention with explanation <a class="citation" href="#pruthi2019learning">(Pruthi et al., 2019; Jain &amp; Wallace, 2019)</a>, it can offer plausible and meaningful interpretations <a class="citation" href="#wiegreffe2019attention">(Wiegreffe &amp; Pinter, 2019; Vashishth et al., 2019; Vig, 2019)</a>.
In this post, we focus on problems arising when we move to the higher layers of a model, due to lack of token identifiability of the embeddings in higher layers <a class="citation" href="#brunner2019validity">(Brunner et al., 2020)</a>. We propose visualising and interpreting attention weights taking this phenomena into account!</p>

<!--more-->

<p>We propose two simple but effective methods, <strong>Attention Rollout</strong> and <strong>Attention Flow</strong>, to compute attention scores to input tokens  (i.e., <em>token attention</em>) at each layer, by taking raw attentions (i.e., <em>embedding attention</em>) of that layer as well as those from the precedent layers.</p>

<h5 id="attention-to-embeddings-vs-attention-to-input-tokens">Attention to embeddings vs Attention to Input Tokens</h5>
<p>In the Transformer model, <em>self-attention</em> combines information from attended embeddings into the representation of the embeddings in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed.</p>

<p>Hence, when looking at the $i$th self attention layer, we can not interpret the attention weights as the attention to the embeddings in the input layer. Because each embedding in this layer can potentially contain information from all embeddings in the previous layers. This makes attention weights unreliable as explanations probes when they are not directly applied on the input tokens.
<img src="img/flow_images/rat_deep_1.png" alt="Raw Attention Weights" style="height: 360px; float: right" /></p>

<p>Let’s take a look at how self attention layers in a Transformer model change across layers.
If we only look at the attention weights in the last layer, it seems all input tokens have equal contributions to the output of the model, since the attention weights in this layer are uniformly distributed. But if we also take into account the attention weights in the previous layers into account, we realise that some of the input tokens are getting higher attention.</p>

<p>So, if we want to use attention weights to understand how a self attention network works, we need to take the flow of information in the network into account! One way to do this is to use attention weights to approximate the information flow, while taking different aspects of the architecture of the model into account.</p>

<h5 id="information-flow-graph-of-a-transformer-encoder">Information flow graph of a Transformer Encoder</h5>
<p>This is a schematic view of self attention layer in the Transformer Model introduced in <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>:
<!-- _includes/image.html --></p>
<div class="image-wrapper" style="height: 300px; display: block; float:left">
    
        <img src="samiraabnar.github.io/img/flow_images/attention_block.png" alt="" style="height:300px" />
    
    
        <p class="image-caption">Multihead self-attention block</p>
    
</div>

<p>Given this attention module with residual connections, we compute values in layer $l+1$ as $V_{l+1} = V_{l}  + W_{att}V_l$, where $ W_{att}$ is the attention matrix. Thus, we have $V_{l+1} = (W_{att} + I) V_{l}$. So, to account for residual connections, we add an identity matrix to the attention matrix and re-normalize the weights. This results in $A = 0.5W_{att} + 0.5I$, where $A$ is the raw attention updated by residual connections.</p>

<p>We can create the information flow graph of a Transformer model, using this equation as an approximation of how information propagates in the self attention layers. Thus, we can translate the attention weights in each layer to attention to input tokens.</p>

<p>The information flow in the network can be modelled with a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph"><em>DAG</em> (Directed Acyclic Graph)</a>, in which the nodes are input tokens and hidden embeddings, edges are the attentions from the nodes in each layer to those in the previous layer, and the weights of the edges are the attention weights.
Note that, augment this graph with residual connections to better model the connections between input tokens and hidden embedding.</p>

<h5 id="from-attention-to-embeddings-to-attention-to-tokens">From Attention to Embeddings to Attention to Tokens</h5>
<p>Given this graph, based on how we interpret the weights associated with the edges, which are the raw attention weights, we can use different techniques to compute the attention from each node in the graph to the input tokens.</p>

<h6 id="attention-rollout">Attention Rollout</h6>

<p>Assume the attention weights determine  the proportion of the incoming information that can propagate through each link, i.e., the identities of input tokens are linearly combined through the layers based on the attention weights. Then, to compute the attention to input tokens in layer <script type="math/tex">i</script> given all the attention weight in the previous layers, we recursively multiply the attention weights matrices, starting from the input layer up to layer <script type="math/tex">i</script>.</p>

<!-- <div style="width: 700; display:inline-block; clear: right; vertical-align:middle;"> -->
<!-- _includes/image.html -->
<div class="image-wrapper" style="height: 300px; ">
    
        <img src="samiraabnar.github.io/img/flow_images/attention_rollout.gif" alt="" style="height:300px" />
    
    
        <p class="image-caption">Attention Rollout</p>
    
</div>

<!-- </div> -->

<h6 id="attention-flow">Attention Flow</h6>
<p>If we view the attention weights as the capacity of each link, the problem of computing the attention in layer <script type="math/tex">i</script> to the input tokens reduces to the <a href="https://en.wikipedia.org/wiki/Network_flow_problem">network flow problem</a>, where we want to find the maximum flow value from each input token to each position in layer <script type="math/tex">i</script>.<br />
<!-- _includes/image.html --></p>
<div class="image-wrapper" style="height: 300px;">
    
        <img src="samiraabnar.github.io/img/flow_images/attention_flow.gif" alt="" style="height:300px" />
    
    
        <p class="image-caption">Attention Flow</p>
    
</div>

<h5 id="how-it-all-works-in-practice-">How it all works in practice …</h5>

<p><img src="img/bert_example.png" alt="Raw Attention Weights" style="height: 360px; float: right" /></p>

<p>Let’s see an example of how these techniques work in practice!
Applying this technique to a pretrained Bert model, we get some insights on how the models resolve pronouns.
What we do here is to feed the model with a sentence, masking a pronoun. Next, we look at the prediction of the model for the masked word and compare the probabilities assigned to “her” and “his”.</p>

<p>As you can see, in the first example, Attention Rollout and Attention Flow are consistent with each other and the prediction of the model. Whereas, the final layer of Raw Attention does not seem to be consistent with the prediction of the models, and it varies a lot across different layers. In the second example only Attention Flow weights are consistent with the prediction of the model.</p>

<p>To see how it works for other examples, you can take a look at <a href="https://github.com/samiraabnar/attention_flow/blob/master/bert_example.ipynb">this notebook</a>.</p>

<blockquote>

  <p>In our paper, “<a href="https://arxiv.org/abs/2005.00928" title="Quantifying Attention Flow In Transformers">Quantifying Attention Flow In Transformers</a>”, we show that compared to raw attention weights, the token attentions from <kbd>attention rollout</kbd> and <kbd>attention flow</kbd> have higher correlations with the importance scores obtained from input gradients as well as an input ablation based attribution method. Furthermore, we visualise the token attention weights and demonstrate that they are better approximations of how input tokens contribute to a predicted output, compared to raw attention weights.</p>
</blockquote>


  </article>
</div>


<!-- 
<div id="share-box">
<h5>Share this:</h5>

<a id="f" href="https://www.facebook.com/sharer/sharer.php?u=samiraabnar.github.io/articles/2020-05/attention_flow" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-facebook-official fa"></i><span> facebook</span></a>

<a id="t" href="https://twitter.com/intent/tweet?text=&url=samiraabnar.github.io/articles/2020-05/attention_flow" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><i class="fa fa-twitter fa"></i><span> twitter</span></a>

<a id="g" href="https://plus.google.com/share?url=samiraabnar.github.io/articles/2020-05/attention_flow" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i><span> google</span></a>

<a id="r" href="http://www.reddit.com/submit?url=samiraabnar.github.io/articles/2020-05/attention_flow" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-reddit fa"></i><span> reddit</span></a>

<a id="l" href="https://www.linkedin.com/shareArticle?mini=true&url=samiraabnar.github.io/articles/2020-05/attention_flow&title=&summary=&source=webjeda" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-linkedin fa"></i><span> linkedin</span></a>

<a id="e" href="mailto:?subject=&amp;body=Check out this site samiraabnar.github.io/articles/2020-05/attention_flow"><i class="fa fa-envelope fa"></i><span> email</span></a>
</div>
 -->

<div id="post">
  <h4>References</h4>
<ol class="bibliography"><li><span id="bahdanau2014neural">Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to Align and Translate. <i>Proceedings of the 2015 International Conference On
Learning Representations</i>.</span></li>
<li><span id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). Attention is all you need. <i>Advances in Neural Information Processing Systems</i>, 5998–6008.</span></li>
<li><span id="pruthi2019learning">Pruthi, D., Gupta, M., Dhingra, B., Neubig, G., &amp; Lipton, Z. C. (2019). Learning to deceive with attention-based explanations. <i>ArXiv Preprint ArXiv:1909.07913</i>.</span></li>
<li><span id="jain2019attention">Jain, S., &amp; Wallace, B. C. (2019). Attention is not Explanation. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>, 3543–3556. https://doi.org/10.18653/v1/N19-1357</span></li>
<li><span id="wiegreffe2019attention">Wiegreffe, S., &amp; Pinter, Y. (2019, November). Attention is not not Explanation. <i>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</i>. https://www.aclweb.org/anthology/D19-1002</span></li>
<li><span id="vashishth2019attention">Vashishth, S., Upadhyay, S., Tomar, G. S., &amp; Faruqui, M. (2019). Attention interpretability across nlp tasks. <i>ArXiv Preprint ArXiv:1909.11218</i>.</span></li>
<li><span id="vig2019visualizing">Vig, J. (2019). Visualizing Attention in Transformer-Based Language models. <i>ArXiv Preprint ArXiv:1904.02679</i>.</span></li>
<li><span id="brunner2019validity">Brunner, G., Liu, Y., Pascual, D., Richter, O., Ciaramita, M., &amp; Wattenhofer, R. (2020). On Identifiability in Transformers. <i>International Conference on Learning Representations</i>. https://openreview.net/forum?id=BJg1f6EFDB</span></li></ol>
</div>



        <footer>
  &copy; 2020 Samira Abnar.
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="samiraabnar.github.io/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="samiraabnar.github.io/js/main.js"></script>


</body>
</html>
