<!DOCTYPE html>
<html lang="en">






<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="In this post we try to understand the nature of recurrent inductive bias. We provide empirical results to demonstrate the benefits of different sources of in...">
  <meta name="keywords" content="blog">
  <meta name="author" content="On the Merits of Recurrent Inductive Bias | Samira Abnar">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="On the Merits of Recurrent Inductive Bias | Samira Abnar">
  <meta name="twitter:description" content="In this post we try to understand the nature of recurrent inductive bias. We provide empirical results to demonstrate the benefits of different sources of in...">
  
    <meta property="twitter:image" content="https://samiraabnar.github.io/img/rnn_images/rnn-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="https://samiraabnar.github.io/articles/2020-05/recurrence">
  <meta property="og:title" content="On the Merits of Recurrent Inductive Bias | Samira Abnar">
  <meta property="og:description" content="In this post we try to understand the nature of recurrent inductive bias. We provide empirical results to demonstrate the benefits of different sources of in...">
  
    <meta property="og:image" content="https://samiraabnar.github.io/img/rnn_images/rnn-logo.png">
  
  <title>On the Merits of Recurrent Inductive Bias | Samira Abnar</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="https://samiraabnar.github.io/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://samiraabnar.github.io/css/main.css">

  <link rel="canonical" href="https://samiraabnar.github.io/articles/2020-05/recurrence">
  <link rel="alternate" type="application/rss+xml" title="Samira Abnar" href="https://samiraabnar.github.io/feed.xml" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="https://samiraabnar.github.io/blown.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="https://samiraabnar.github.io/blown.png">
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="/">
    <img src="/img/samira-logo.jpg" alt="" class="avatar">
  </a>
  
  <a href="/" class="author_name">Samira Abnar</a>
  <span class="author_job">PhD Candidate</span>
  <span class="author_bio mbm">University of Amsterdam</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="/">blog</a>
      </li>
       
      <li class="nav-item">
        <a href="/about">About</a>
      </li>
        
      <li class="nav-item">
        <a href="/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="/tags/">Tags</a>
      </li>
         
    </ul>
  </nav>
  <script type="text/javascript">
  // based on http://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
    <li><a href="http://twitter.com/samiraabnar#username" class="social-link-item" target="_blank"><i class="fa fa-fw fa-twitter"></i></a></li>
    
    
    <li><a href="http://linkedin.com/in/samiraabnar" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    <li><a href="http://instagram.com/samiraabnar" class="social-link-item" target="_blank"><i class="fa fa-fw fa-instagram"></i></a></li>
    
    <li><a href="http://github.com/samiraabnar" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <!-- 
<div class="post-image-feature">
  <img class="feature-image" src=
  
  "https://samiraabnar.github.io/img/rnn_images/rnn-logo.png"
  
  alt="On the Merits of Recurrent Inductive Bias feature image">

  
</div>
 -->


<div id="post">
  <header class="post-header">
    <h1 title="On the Merits of Recurrent Inductive Bias">On the Merits of Recurrent Inductive Bias</h1>
    <span class="post-meta">
      <span class="post-date">
        26 MAY 2020
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    4 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <p>LSTMs <a class="citation" href="#hochreiter1997long">(Hochreiter &amp; Schmidhuber, 1997)</a> and Transformers <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a> are the basic building blocks of many state-of-the-art models for sequence modeling and natural language processing.  While Transformers do extremely well on many tasks where the training data is adequate in quantity <a class="citation" href="#Devlin2019BERTPO">(Devlin et al., 2019; Keskar et al., 2019; Radford et al., 2019)</a>, several studies have shown that LSTMs can perform  better than  Transformers on tasks requiring sensitivity to (linguistic) structure, specially when the data is limited <a class="citation" href="#tran-etal-2018-importance">(Tran et al., 2018; Dehghani et al., 2019)</a>. This is mainly due to the recurrent inductive biases of LSTMs that helps them better model the hierarchical structure of the inputs. <br />
While theoretically, both recurrent neural networks (RNNs) and Transformers can deal with finite hierarchical structures, empirical results indicate superiority of RNNs over Transformers <a class="citation" href="#tran-etal-2018-importance">(Tran et al., 2018; Dehghani et al., 2019; Hahn, 2020)</a>.
The superior performance of LSTMs over Transformers in these cases is attributed to their recurrent inductive bias. The recurrent inductive bias of LSTMs enables them to model hierarchical structure of the inputs, which is of crucial importance in the subject-verb agreement task <a class="citation" href="#tran-etal-2018-importance">(Tran et al., 2018)</a>.</p>

<h4 id="what-is-recurrent-inductive-bias">What is recurrent inductive bias?</h4>

<p>The inductive bias of RNNs is often referred to as the <strong>recurrent inductive bias</strong>.
Here, we distinguish between three main sources of this bias:</p>
<ol>
  <li><kbd>The sequential processing of the input</kbd>: There is an inherent notion of order in the architecture that forces the model to access next tokens in the input one by one;</li>
  <li><kbd>No direct access to the past tokens</kbd>: The model has to compress all the information from past tokens in a hidden state, which is accessible when processing a new token;</li>
  <li><kbd>Recursion</kbd>: The model recursively applies the same function on the varying input at every time step.</li>
</ol>

<p>Transformers, in contrast, process the input in parallel. Although a weak notion of order is encoded by positional embeddings, no explicit assumption is made in the connectivity structure of the architecture. Moreover, they have a global receptive field and can access all tokens through self-attention. Finally, standard Transformers are not recursive.</p>

<p>We can empirically examine the benefits of each of these biases. To do so, we will modify the standard Transformer to have an architecture with specifications that are similar to RNNs and we measure how the performance of the models change as we include more aspects of the recurrent inductive bias.</p>

<!--
![rnn](img/rnn_images/rnn.png){:width="250px" style="float: left;"}
![transformer](img/rnn_images/transformer.png){:width="250px" style="float: left; margin-right:10px"}
![sequential transformer](img/rnn_images/seqtrans.png){:width="250px" style="float: left; margin-right:10px"}
![sequential universal transformer](img/rnn_images/utrans.png){:width="250px" style="margin-right:10px"} -->

<p style="">
<img src="img/rnn_images/rnn.png" alt="rnn" width="250px" style="float: left;" />
<img src="img/rnn_images/transformer.png" alt="transformer" width="250px" style="margin-right:10px" />
</p>

<p style="">
<img src="img/rnn_images/seqtrans.png" alt="sequential transformer" width="250px" style="float: left; margin-right:10px" />
<img src="img/rnn_images/utrans.png" alt="sequential universal transformer" width="250px" style="margin-right:10px" />
</p>

<p>We employ a standard two-layer unidirectional LSTM and three different variants of Transformers:
(1) transformer: a standard six-layer Transformer encoder with a class token (<kbd>CLS</kbd>) for classification (BERT style),
(2) sequential transformer: a standard six-layer Transformer encoder with future masking where the classification is done using the representation of the last token\footnote{Note that future tokens are masked out by default when using transformer in the decoder mode, e.g., in a language modeling setup.},
(3) sequential universal transformer: a six-layer Universal Transformer (missing reference) as a proxy for assessing the ability of models to capture hierarchical structure in natural language.
In this task, examples are grouped into different levels of difficulty based on the number of “agreement attractors”<sup id="fnref:c6e920ab"><a href="#fn:c6e920ab" class="footnote">1</a></sup>, and “distance” between the verb and its subject.</p>

<p>We train these models to predict the number of the masked verb in a given sentence. As we can see in the plot blow, LSTM achieves the best performance. Interestingly, comparing all four models, we find that the performance steadily increases as more aspects of the recurrent inductive bias are included.
<img src="img/rnn_images/accuracy.png" alt="" width="360px" /></p>

<p>As another indicator of the quality of the solutions that different models converged to in the classification setup, we look into their confidence calibration<sup id="fnref:af15b0ee"><a href="#fn:af15b0ee" class="footnote">2</a></sup>.</p>

<p>In figure below, we plot the Expected Calibration Error (ECE) of the models. In line with the trends in the performances of these models, the expected calibration error decreases as we move from standard Transformer toward LSTM.
<img src="img/rnn_images/ece.png" alt="" width="360px" /></p>

<p>Additionally, as shown in both above figures, we find a decreasing trend in the variance of the models, i.e., adding more inductive biases to the models decreases their variance. This is empirical evidence that supports the relation between variance of the solutions a model converges to and its inductive biases.</p>
<div class="footnotes">
  <ol>
    <li id="fn:c6e920ab">
      <p>Agreement attractors are intervening nouns with a different number than the number of the subject. E.g., given the input <kbd>The <b>keys</b> to the <b>cabinet</b> {is?/are?}.</kbd>’’, the word <kbd>cabinet</kbd> is an agreement attractor. <a href="#fnref:c6e920ab" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:af15b0ee">
      <p>Confidence calibration captures how well likelihood (confidence) of the prediction of the model predicts its accuracy. For a well-calibrated model, if we bin the confidence scores and compute the accuracy for each bin, the accuracies are perfectly correlated with the confidence values. The Expected Calibration Error (ECE) is computed as the distance between the calibration curve of the model and the perfect calibration curve. <a href="#fnref:af15b0ee" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </article>
</div>


<!-- 
<div id="share-box">
<h5>Share this:</h5>

<a id="f" href="https://www.facebook.com/sharer/sharer.php?u=/articles/2020-05/recurrence" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-facebook-official fa"></i><span> facebook</span></a>

<a id="t" href="https://twitter.com/intent/tweet?text=&url=/articles/2020-05/recurrence" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><i class="fa fa-twitter fa"></i><span> twitter</span></a>

<a id="g" href="https://plus.google.com/share?url=/articles/2020-05/recurrence" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i><span> google</span></a>

<a id="r" href="http://www.reddit.com/submit?url=/articles/2020-05/recurrence" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-reddit fa"></i><span> reddit</span></a>

<a id="l" href="https://www.linkedin.com/shareArticle?mini=true&url=https://samiraabnar.github.io/articles/2020-05/recurrence&title=&summary=&source=webjeda" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-linkedin fa"></i><span> linkedin</span></a>

<a id="e" href="mailto:?subject=&amp;body=Check out this site /articles/2020-05/recurrence"><i class="fa fa-envelope fa"></i><span> email</span></a>
</div>
 -->

<div id="post">
  <h4>References</h4>
<ol class="bibliography"><li><span id="hochreiter1997long">Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. <i>Neural Computation</i>, <i>9</i>(8), 1735–1780. https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735</span></li>
<li><span id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). Attention is all you need. <i>Advances in Neural Information Processing Systems</i>, 5998–6008.</span></li>
<li><span id="Devlin2019BERTPO">Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i>. https://arxiv.org/abs/1810.04805</span></li>
<li><span id="keskar2019ctrl">Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., &amp; Socher, R. (2019). Ctrl: A conditional transformer language model for controllable generation. <i>ArXiv Preprint ArXiv:1909.05858</i>. https://arxiv.org/abs/1909.05858</span></li>
<li><span id="radford2019language">Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. <i>OpenAI Blog</i>, <i>1</i>(8).</span></li>
<li><span id="tran-etal-2018-importance">Tran, K., Bisazza, A., &amp; Monz, C. (2018). The Importance of Being Recurrent for Modeling Hierarchical Structure. <i>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</i>. https://www.aclweb.org/anthology/D18-1503</span></li>
<li><span id="universaltrans">Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., &amp; Kaiser, L. (2019). Universal Transformers. <i>Proceedings of the 7th International Conference on Learning Representations</i>. https://arxiv.org/abs/1807.03819</span></li>
<li><span id="Hahn-2019-arxiv">Hahn, M. (2020). Theoretical Limitations of Self-Attention in Neural Sequence Models. <i>Transactions of the Association for Computational Linguistics</i>, <i>8</i>.</span></li></ol>
</div>



        <footer>
  &copy; 2020 Samira Abnar.
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="https://samiraabnar.github.io/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="https://samiraabnar.github.io/js/main.js"></script>


</body>
</html>
